---
title: "Final homework"
author: "Zhihong Mai"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ Final homework }
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
## homework1

Use knitr to produce at least 3 examples(texts,figures,tables)

## answer1

example1(texts):

There is a book over there.

example3(table):

department  | man | women 
------|------|------
advertisement | 15 | 35
technology | 65 | 25

## homework2

3.4:

Develop an algorithm to generate random samples from a Rayleigh($\sigma$) distribution. Generate Rayleigh(σ) samples for several choices of $\sigma$ > 0 and check that the mode of the generated samples is close to the theoretical mode $\sigma$(check the histogram).

answer:

after the integrate of $f(x)$ we take the cdf $F_{X}(x)=1-e^{-x^2/2\sigma^2} (x \geq 0,\sigma >0)$

then we use Inverse function exchange to $u=F_{X}(x)$,$x=F^{-1}(u)=\sqrt{-2\sigma^2ln(1-u)}$

set$\sigma=5$

```{r,echo=TRUE}
sigma<-5
n<-1000
u<-runif(n,min=0,max = 0.99)
x<-sqrt(-2*sigma^2*log(1-u))
hist(x,prob=TRUE)
y<-seq(0,15,.1)
lines(y,(y/sigma^2)*exp(-y^2/(2*sigma^2)))
```

set$\sigma=2$

```{r,echo=TRUE}
sigma<-2
n<-1000
u<-runif(n,min=0,max = 0.99)
x<-sqrt(-2*sigma^2*log(1-u))
hist(x,prob=TRUE)
y<-seq(0,15,.1)
lines(y,(y/sigma^2)*exp(-y^2/(2*sigma^2)))
```

set$\sigma=1$

```{r,echo=TRUE}
sigma<-1
n<-1000
u<-runif(n,min=0,max = 0.99)
x<-sqrt(-2*sigma^2*log(1-u))
hist(x,prob=TRUE)
y<-seq(0,15,.1)
lines(y,(y/sigma^2)*exp(-y^2/(2*sigma^2)))
```

we can know the inverse function is useful

3.11:

Generate a random sample of size 1000 from a normal location mixture. The components of the mixture have N(0, 1) and N(3, 1) distributions with mixing probabilities p1 and p2 = 1 − p1. Graph the histogram of the sample with density superimposed, for p1 = 0.75. Repeat with different values for p1 and observe whether the empirical distribution of the mixture appears to be bimodal. Make a conjecture about the values of p1 that produce bimodal mixtures.

answer:

set$F_{1}$~$N(0,1)$,$F_{2}$~$N(3,1)$,then the mixture distribution of CDF为$F=p_{1}F_{1}+p_{2}F_{2}$

when,p1=0.75

```{r,echo=TRUE}
n<-1000
p1<-0.75
p2<-1-p1
X1<-rnorm(n,mean=0,sd=1)
X2<-rnorm(n,mean=3,sd=1)
r<-sample(c(0,1),size=n,prob = c(p2,p1),replace = TRUE)
Z<-r*X1+(1-r)*X2
hist(Z)
```

when,p1=0.5

```{r,echo=TRUE}
n<-1000
p1<-0.5
p2<-1-p1
X1<-rnorm(n,mean=0,sd=1)
X2<-rnorm(n,mean=3,sd=1)
r<-sample(c(0,1),size=n,prob = c(p1,p2),replace = TRUE)
Z<-r*X1+(1-r)*X2
hist(Z)
```

Through comparison, it can be found that when P1 is 0.5, there is an obvious bimodal phenomenon. When P1 is not 0.5, the bimodal tendency will be reduced and more inclined to the party with greater probability, and the more deviation from 0.5, the more obvious the deviation, and the lower the bimodal tendency. Obviously, when P1 is 0 or 1, it will be simplified to a single normal distribution

# homework 3

5.4

Write a function to compute a Monte Carlo estimate of the Beta(3, 3) cdf,and use the function to estimate F(x) for x = 0.1, 0.2,..., 0.9. Compare the estimates with the values returned by the pbeta function in R.

answer:

using MC to estimate the quantile of beta distribution as follow

```{r,echo=TRUE}
set.seed(9711)
m<-1e4
x<-c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9)
y<-c(0,0,0,0,0,0,0,0,0)
Y<-rbeta(m,shape1 = 3,shape2 = 3)
for (i in 1:9) {
  y[i]<-mean(x[i]>Y)
}
print(y)
```

actually,the quantile of the normal beta distribution is follow

```{r,echo=TRUE}
y2<-pbeta(x,shape1 = 3,shape2 = 3)
print(y2)
```

5.9:

The Rayleigh density is 

$f(x)=\frac{x}{\sigma^{2}} e^{-x^{2} /\left(2 \sigma^{2}\right)}, \quad x \geq 0, \sigma>0$

Implement a function to generate samples from a Rayleigh(σ) distribution,using antithetic variables. What is the percent reduction in variance of $\frac{X+X^{'}}{2}$ compared with $\frac{X_{1}+X_{2}}{2}$ for independent $X_{1}$, $X_{2}$?

answer:

integrate the pdf can get cdf as follow

$F(x)=1-e^{-x^2/(2\sigma^2)},x \geq 0 ,\sigma\geq 0$

let$\sigma=1$

```{r,echo=TRUE}
set.seed(930)
m<-1e4
sigma<-1
U<-runif(m)
V<-runif(m)
U1<-runif(m)
U2<-1-U1
X<-sqrt(-2*sigma^2*log(1-U1))
Xanti<-sqrt(-2*sigma^2*log(1-U2))
Y1<-(X+Xanti)/2
X1<-sqrt(-2*sigma^2*log(1-U))
X2<-sqrt(-2*sigma^2*log(1-V))
Y2<-(X1+X2)/2
print(c(sd(Y1),sd(Y2),sd(Y1)/sd(Y2)))
```

it show that the standard error of
duality method is less than the standard error of average of i.i.d random variable,nearly 23%

5.13:

Find two importance functions f1 and f2 that are supported on $(1, \infty)$ and are ‘close’ to 

$g(x)=\frac{x^{2}}{\sqrt{2 \pi}} e^{-x^{2} / 2}, \quad x>1$

Which of your two importance functions should produce the smaller variance in estimating

$\int_{1}^{\infty} \frac{x^{2}}{\sqrt{2 \pi}} e^{-x^{2} / 2} d x$

by importance sampling? Explain.

answer:

let $f1=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$ and $f2=x^{-2}$

using f1 as follows

```{r,echo=TRUE}
set.seed(5678)
M<-1e4
U1<-runif(M)
X1<-qnorm(U1)
Y1<-mean(X1^2*(X1>1))
VAR1<-var(X1^2*(X1>1))
print(Y1)
print(VAR1)
```

using f2 as follows

```{r,echo=TRUE}
U2<-runif(M)
X2<-1/(1-U2)
Y2<-mean(X2^4*exp(-X2^2)/sqrt(2*pi))
VAR2<-var(X2^4*exp(-X2^2)/sqrt(2*pi))
print(Y2)
print(VAR2)
```

actually,using f1 and f2 to calculate  is not only  different in result but also in the square error.The biggest reason is that the integrate of f1 in $(1,\infty)$ is not 1(that means $(1,\infty)$ is not the support section of f1)

# homework 4

6.5:

Suppose a 95% symmetric t-interval is applied to estimate a mean, but the sample data are non-normal. Then the probability that the confidence interval covers the mean is not necessarily equal to 0.95. Use a Monte Carlo experiment to estimate the coverage probability of the t-interval for random samples of χ2(2) data with sample size n = 20. Compare your t-interval results with the simulation results in Example 6.4.

answer

```{r,echo=TRUE}
set.seed(1014)
size<-20
x<-rchisq(n=size,df=2)
mu_hat<-mean(x)
se_hat<-sd(x)
citop<-mu_hat+qt(p=0.975,df=size-2)*se_hat
cilow<-mu_hat-qt(p=0.975,df=size-2)*se_hat
print(cilow)
print(citop)
cp<-mean((x>=cilow)*(x<=citop))
print(cp)
```

The above is the upper and lower bounds of the confidence interval and the coverage of the confidence interval obtained by using the T confidence interval with a confidence of 0.95 for the chi square distribution random sample with a sample size of 20 degrees of freedom and 2. It can be seen that the actual coverage is 1, that is, when the sample distribution deviates from the normal distribution, using the T confidence interval will lead to excessive coverage and poor estimation effect

6.A

Use Monte Carlo simulation to investigate whether the empirical Type I error rate of the t-test is approximately equal to the nominal significance level $\alpha$, when the sampled population is non-normal. The t-test is robust to mild departures from normality. Discuss the simulation results for the cases where the sampled population is (i) χ2(1), (ii) Uniform(0,2), and (iii) Exponential(rate=1). In each case, test $H_{0} : µ = µ_{0}$ vs $H_{1} : µ \neq µ_{0}$, where µ0 is the mean of χ2(1), Uniform(0,2), and Exponential(1), respectively.

answer

let $\alpha=0.95$
(i)
```{r,echo=TRUE}
set.seed(1014)
size<-1000
x1<-rchisq(n=size,df=1)
mu_hat<-mean(x)
se_hat<-sd(x)
citop<-mu_hat+qt(p=0.975,df=size-2)*se_hat
cilow<-mu_hat-qt(p=0.975,df=size-2)*se_hat
print(cilow)
print(citop)
cp<-mean((x>=cilow)*(x<=citop))
print(cp)
```
(ii)
```{r,echo=TRUE}
set.seed(1014)
size<-20
x<-runif(n=size,min=0,max = 2)
mu_hat<-mean(x)
se_hat<-sd(x)
citop<-mu_hat+qt(p=0.975,df=size-2)*se_hat
cilow<-mu_hat-qt(p=0.975,df=size-2)*se_hat
print(cilow)
print(citop)
cp<-mean((x>=cilow)*(x<=citop))
print(cp)
```
(iii)
```{r,echo=TRUE}
set.seed(1014)
size<-20
x<-rexp(n=size,rate=1)
mu_hat<-mean(x)
se_hat<-sd(x)
citop<-mu_hat+qt(p=0.975,df=size-2)*se_hat
cilow<-mu_hat-qt(p=0.975,df=size-2)*se_hat
print(cilow)
print(citop)
cp<-mean((x>=cilow)*(x<=citop))
print(cp)
```

It can be seen that the first type error probability of (I) and (II) distribution using t-confidence interval is 0, while (III) distribution using t-confidence interval is 0.05. Obviously (I) and (II) are not suitable for using the T confidence interval

# homework 5

6.C

Repeat Examples 6.8 and 6.10 for Mardia’s multivariate skewness test. Mardia proposed tests of multivariate normality based on multivariate generalizations of skewness and kurtosis. If X and Y are iid, the multivariate population skewness $\beta_{1,d}$ is defined by Mardia as

$\beta_{1, d}=E\left[(X-\mu)^{T} \Sigma^{-1}(Y-\mu)\right]^{3}$

Under normality, $\beta_{1,d}$ = 0. The multivariate skewness statistic is

$b_{1, d}=\frac{1}{n^{2}} \sum_{i, j=1}^{n}\left(\left(X_{i}-\bar{X}\right)^{T} \widehat{\Sigma}^{-1}\left(X_{j}-\bar{X}\right)\right)^{3}$

where $\widehat{\Sigma}$ is the maximum likelihood estimator of covariance. Large values of $\beta_{1,d}$ are significant. The asymptotic distribution of $n\beta_{1,d}/6$ is chisquared with d(d + 1)(d + 2)/6 degrees of freedom.

## answer

use the combination distribution of three indendent normal random variable with average 1、3、5 and square error 1、4、9

```{r,echo=TRUE}
library(MASS)
set.seed(12346)
n<-1000
mean<-c(1,3,5)
sigma<-matrix(c(1,0,0,0,2,0,0,0,3),nrow = 3,ncol = 3)
cv<-qnorm(0.975,mean = mean,c(sqrt(6/n),sqrt(6/n),sqrt(6/n)))
sk<-function(x,i){
  xbar<-mean(x[i])
  m3 <- mean((x[i] - xbar)^3)
  m2 <- mean((x[i] - xbar)^2)
  return( m3 / m2^1.5 )
}
sktest<-numeric(n)
for (j in 1:n) {
  x<-mvrnorm(mu=mean,Sigma = sigma)
  skr<-sk(x)
  sktest[j]<-1
  for (k in 1:3) {
    sktest[j]<-sktest[j]*(abs(sk(x[k]))>=cv[k])
  }
}
p.project<-mean(sktest)
print(p.project)
```

# homework 6

7.7

Refer to Exercise 7.6. Efron and Tibshirani discuss the following example [84,Ch. 7]. The five-dimensional scores data have a 5 × 5 covariance matrix $\Sigma$,with positive eigenvalues $\lambda_{1}>\cdots>\lambda_{5}$. In principal components analysis,

$\theta=\frac{\lambda_{1}}{\sum_{j=1}^{5} \lambda_{j}}$

measures the proportion of variance explained by the first principal component.Let $\hat{\lambda}_{1}>\cdots>\hat{\lambda}_{5}$ be the eigenvalues of $\hat{\Sigma}$, where $\hat{\Sigma}$ is the MLE of $\Sigma$. Compute the sample estimate

$\hat{\theta}=\frac{\hat{\lambda}_{1}}{\sum_{j=1}^{5} \hat{\lambda}_{j}}$

answer

```{r,echo=TRUE}
library(bootstrap)
library(boot)
set.seed(808)
x<-cov(scor)
ev<-eigen(x)
theta<-function(x,i){
    x[i]/sum(x)
}
obj<-boot(data = ev$values,statistic = theta,R=2000)
round(c(bias=mean(obj$t)-obj$t0,se=sd(obj$t)),2)
```

bias1 and se is the bias and standard error of $\hat\theta$

7.8 

Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard error of $\hat \theta$

answer

```{r,echo=TRUE}
set.seed(2021)
theta.jack<-numeric(5)
theta.hat<-theta(ev$values)[1]
for (i in 1:5) {
  theta.jack[i]<-theta(ev$values[-i])[1]
}
bias.jack <- (5-1)*(mean(theta.jack)-theta.hat)
se.jack <- sqrt((5-1)*mean((theta.jack-theta.hat)^2))
round(c(original=theta.hat,bias.jack=bias.jack,
se.jack=se.jack,se.boot=sd(obj$t)),3)
```

# homework 7

8.2

Implement the bivariate Spearman rank correlation test for independence as a permutation test. The Spearman rank correlation test statistic can be obtained from function cor with method = "spearman". Compare the achieved significance level of the permutation test with the p-value reported by cor.test on the same samples.

answer 

use two i.i.d standard normal distribution
```{r,echo=TRUE}
set.seed(1104)
N<-1000
R<-999
x<-rnorm(N)
y<-rnorm(N)
z<-c(x,y)
K<-1:(2*N)
reps<-numeric(R)
cor0<-cor.test(x,y,method = "spearman")$statistic
for (i in 1:R) {
  k<-sample(K,size = N,replace = FALSE)
  x1<-z[k]
  y1<-z[-k]
  reps[i]<-cor.test(x1,y1,method = "spearman")$statistic
}
p<-mean(abs(c(cor0,reps))>=abs(cor0))
round(c(p,cor.test(x,y,method = "spearman")$p.value),3)
```

question

Design experiments for evaluating the performance of the NN,energy, and ball methods in various situations

Unequal variances and equal expectations

Unequal variances and unequal expectations

Non-normal distributions: t distribution with 1 df (heavy-tailed distribution), bimodel distribution (mixture of two normal distributions)

Unbalanced samples (say, 1 case versus 10 controls)

Note: The parameters should be chosen such that the powers are distinguishable (say, range from 0.3 to 0.8).

answer

with equal average and different square error，take two distribution as N(0,1) and N(0,9)

(1)Unequal variances and equal expectations

```{r,echo=TRUE}
library(RANN)
library(boot)
library(energy)
library(Ball)
set.seed(12345)
n<-1000
x<-rnorm(n)
y<-rnorm(n,mean = 0,sd=3)
z<-c(x,y)
Tn <- function(z, ix, sizes,k) {
n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
if(is.vector(z)){
  z<-data.frame(z,0)
}
z <- z[ix, ]
NN <- nn2(data=z, k=k+1)
block1 <- NN$nn.idx[1:n1,-1]
block2 <- NN$nn.idx[(n1+1):n,-1]
i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5)
(i1 + i2) / (k * n)
}
N<-c(length(x),length(y))
boot.nn<-boot(data=z,statistic = Tn,R=999,sim = "permutation",sizes=N,k=3)
boot.energy<-eqdist.etest(z,sizes=N,R=999)
nn.ts<-c(boot.nn$t0,boot.nn$t)
nn.p.value<-mean(nn.ts>=nn.ts[1])
energy.p.value<-boot.energy$p.value
Ball<-bd.test(x = x, y = y, num.permutations=999)
Ball.p.value<-Ball$p.value
round(c(nn.p.value,energy.p.value,Ball.p.value),3)
```

(2)Unequal variances and unequal expectations

use N(0,1) and N(1,9)
```{r,echo=TRUE}
library(RANN)
library(boot)
library(energy)
library(Ball)
set.seed(1107)
n<-1000
x<-rnorm(n)
y<-rnorm(n,mean = 1,sd=3)
z<-c(x,y)
Tn <- function(z, ix, sizes,k) {
n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
if(is.vector(z)){
  z<-data.frame(z,0)
}
z <- z[ix, ]
NN <- nn2(data=z, k=k+1)
block1 <- NN$nn.idx[1:n1,-1]
block2 <- NN$nn.idx[(n1+1):n,-1]
i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5)
(i1 + i2) / (k * n)
}
N<-c(length(x),length(y))
boot.nn<-boot(data=z,statistic = Tn,R=999,sim = "permutation",sizes=N,k=3)
boot.energy<-eqdist.etest(z,sizes=N,R=999)
nn.ts<-c(boot.nn$t0,boot.nn$t)
nn.p.value<-mean(nn.ts>=nn.ts[1])
energy.p.value<-boot.energy$p.value
Ball<-bd.test(x = x, y = y, num.permutations=999)
Ball.p.value<-Ball$p.value
round(c(nn.p.value,energy.p.value,Ball.p.value),3)
```

(3)Non-normal distributions: t distribution with 1 df (heavy-tailed distribution), bimodel distribution (mixture of two normal distributions)

```{r,echo=TRUE}
library(RANN)
library(boot)
library(energy)
library(Ball)
set.seed(1190)
n<-1000
x<-rt(n,df=1)
y1<-rnorm(n,mean = 0,sd=3)
y2<-rnorm(n)
p<-runif(n)
y<-p*y1+(1-p)*y2
z<-c(x,y)
Tn <- function(z, ix, sizes,k) {
n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
if(is.vector(z)){
  z<-data.frame(z,0)
}
z <- z[ix, ]
NN <- nn2(data=z, k=k+1)
block1 <- NN$nn.idx[1:n1,-1]
block2 <- NN$nn.idx[(n1+1):n,-1]
i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5)
(i1 + i2) / (k * n)
}
N<-c(length(x),length(y))
boot.nn<-boot(data=z,statistic = Tn,R=999,sim = "permutation",sizes=N,k=3)
boot.energy<-eqdist.etest(z,sizes=N,R=999)
nn.ts<-c(boot.nn$t0,boot.nn$t)
nn.p.value<-mean(nn.ts>=nn.ts[1])
energy.p.value<-boot.energy$p.value
Ball<-bd.test(x = x, y = y, num.permutations=999)
Ball.p.value<-Ball$p.value
round(c(nn.p.value,energy.p.value,Ball.p.value),3)
```


